# Assignment 1: Data Collection using Active Measuremetns

**Goal:** Understand  *data collection aspects of networking using active measurements*, with a focus on **application classification and application QoE monitoring**. You will generate traffic, collect packet-level traces (pcap), and measure application-level QoE metrics under **controlled and diverse network conditions**.

---

**Background: ** As discussed in class, there are two broad paradigms for network data collection:

* **Passive monitoring:** Observe traffic generated by others (e.g., ISP monitoring).
* **Active measurement:** Generate traffic and measure performance (e.g., probing, controlled experiments).

In this assignment, you will explore **active data collection**. A key challenge with active measurements is achieving **diversity in network conditions**. To address this, you will use Linux `traffic control` to emulate different bandwidth, delay, and loss conditions. Another challenge is related to the manual effort corresponding to running the applicaton and network data collection pipelines. To address this, you will automate the application data collection using a browser-based automation tool (e.g., `Selenium` or `Puppeteer`) and network data collection (e.g.,  `tcpdump`). 

---

## Applications Under Study

You will collect data for **three applications**, each with different interaction and performance characteristics:


1. **Web browsing:** Popular websites
2. **Conversational AI:** ChatGPT with different random prompts
3. **Video streaming:** DashJS Player Demo Website (This would enable easy collection of application-level QoE metrics)


For *each application*, you will:

* Collect **packet captures (pcap)**
* Collect **application-specific QoE metrics**
* Repeat experiments under **multiple network conditions**

---

## Part 1: Web Browsing

Experiment Steps:
* You are provided with a list of popular websites. Randomly select **one website** from this list.
* Start emulating network conditions using `tc`.
* Using browser automation tool, automate loading the selected website in a clean browser state (disable or clear cache before each load).
* Collect: Page load time (PLT), pcap traces, HAR logs

_Varying network conditions using `tc` (bandwidth, latency)_: Vary the network bandwidth at 1-second
intervals by picking a random bandwidth between 100 kbps and 4 Mbps and a random latency between 20 ms and 200 ms.

You should collect data for at least **50 experiments**.

---

## Part 2: ChatGPT interaction
Experiment Steps:
* Start emulating network conditions using `tc`.
* Using browser automation tool, automate sending random textual queries to ChatGPT.
* Collect: Response time for each query, pcap traces, HAR logs

_Varying network conditions using `tc` (bandwidth, latency)_: Vary the network bandwidth using similar approach as Part 1.

You should collect data for at least **50 experiments**.


## Part 3: Video Streaming (DashJS Player Demo Website)
We will use dashjs player demo website to collect video streaming QoE metrics. The website provides a list of sample videos and displays player-level metrics such as resolution, buffer occupancy, and startup delay. You should curate a list of at least 5 different videos to test.

Experiment Steps:
* Start emulating network conditions using `tc`.
* Using browser automation tool, automate playback of one of the randomly selected videos on the DashJS Player Demo Website.
* Stream the video for a fixed duration of 2 minutes.
* Collect: pcap traces, HAR logs, player-level metrics (resolution, buffer occupancy). Use the player-level logs to compute QoE metrics, namely average resolution and rebuffering ratio (ratio of re-buffering duration and total playback time).

_Varying network conditions using `tc` (bandwidth, latency)_: Vary the network bandwidth using similar approach as Part 1.

You should collect data for at least **50 experiments**.


---

## Data Analysis
While you will not perform any ML modeling in this assignment, you will conduct preliminary data analysis. The goal of this analysis is to understand how network traffic characteristics relate to both the application type and the application-level QoE metrics.

### Network Traffic Feature Extraction
- You will use `tshark` or `scapy` to process the pcap files and extract flow-level features.
- You will first have to filter the pcap files to filter only relevant flows from the traffic. You can make your own assumptions on how to identify flows corresponding to each application (e.g., based on destination IPs, ports, protocols).
- For each TCP flow, extract the following features from the pcap traces, extract mean IAT, std IAT, mean packet size, std packet size, flow throughput, flow duration. Feel free to extract additional features you deem relevant.

### Correlation Analysis
- Application Classification: Analyze how well the extracted network traffic features can distinguish between the three application types (web browsing, ChatGPT, video streaming). You can use visualization techniques (e.g., PCA, t-SNE) to explore feature separability.
- QoE Correlation: For each application, analyze how the network traffic features correlate with the application-level QoE metrics (e.g., page load time for web browsing, response time for ChatGPT, buffer occupancy for video streaming). Use statistical analysis and visualization techniques to identify significant correlations.

Report your findings in a concise manner, highlighting key insights from the correlation analysis in a report (`report.pdf`).

---

## Submission Requirements

### Data Organization
Organize your collected data in the following directory structure:

```data/
├── web_browsing/
│   Each experiment data should be in a separate folder named `experiment_<id>/`
│       ├── capture.pcap
│       ├── har_log.har
│       ├── page_load_time.csv
├── chatgpt/
│   Each experiment data should be in a separate folder named `experiment_<id>/`
│       ├── capture.pcap
│       ├── har_log.har
│       ├── response_time.csv
├── video_streaming/
│   Each experiment data should be in a separate folder named `experiment_<id>/`
│       ├── capture.pcap
│       ├── har_log.har
│       ├── player_metrics.csv
└── report.pdf
```

The id should be a combination of timestamp (when the experiment was run), the average throughput, standard deviation of throughput during the experiment, average latency, and standard deviation of latency during the experiment. For example, `experiment_20260127_150004_500kbps_100kbps_100ms_20ms/`. Here the time is in `YYYYMMDD_HHMMSS` format. Throughput and latency values are in `kbps` and `ms` units respectively.

### Code Submission

Submit all scripts used for data collection and analysis in a folder named `code/`. We should be able to run your data collection scripts in the demo.

---

